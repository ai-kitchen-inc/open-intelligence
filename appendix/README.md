# Appendix

This appendix collects supporting material for *The Architecture of Intelligence*. It is intentionally lightweight and expandable.

## Glossary
- **Alignment:** The process of shaping a model's behavior to match human intent, often through preference signals and training objectives.
- **Attention:** A mechanism that computes relevance between tokens, allowing models to link distant concepts in parallel.
- **Context Window:** The maximum span of tokens a model can process in a single pass.
- **DPO (Direct Preference Optimization):** A training method that directly increases the likelihood of preferred responses without an explicit reward model.
- **Latent Space:** The high-dimensional internal representation where concepts become directions and clusters.
- **Mixture of Experts (MoE):** An architecture that routes tokens to specialized subnetworks for efficient scaling.
- **Parameters:** The learned weights in a model that store compressed relationships from data.
- **RAG (Retrieval-Augmented Generation):** A technique that adds external retrieval to improve factual grounding.
- **RLHF (Reinforcement Learning from Human Feedback):** A training pipeline that uses human preference data to align outputs.
- **Scaling Laws:** Empirical rules describing how model performance improves with more data, compute, and parameters.
- **Token:** A chunk of text (word, subword, or character) that a model processes as a unit.

## Reference Index (Seed List)
- *Attention Is All You Need* (Vaswani et al., 2017)
- *Scaling Laws for Neural Language Models* (Kaplan et al., 2020)
- *Training language models to follow instructions with human feedback* (Ouyang et al., 2022)
- *Direct Preference Optimization* (Rafailov et al., 2023)
- *Switch Transformers* (Fedus et al., 2021)
- *Toolformer* (Schick et al., 2023)
- *The Platonic Representation Hypothesis* (Huh et al., 2024)

## Notes
As chapters expand, this appendix will house deeper references, extended explanations, and glossary expansions.
